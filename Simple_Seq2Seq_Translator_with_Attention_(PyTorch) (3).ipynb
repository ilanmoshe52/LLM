{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import unicodedata\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Check for a GPU and use it if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the number of examples to use from the dataset.\n",
        "num_examples = 10000\n",
        "embedding_dim = 256\n",
        "hidden_size = 1024\n",
        "\n",
        "def download_data():\n",
        "    \"\"\"\n",
        "    Downloads a small English-Spanish parallel corpus from a reliable source.\n",
        "\n",
        "    Returns:\n",
        "        str: The file path to the extracted text file.\n",
        "    \"\"\"\n",
        "    print(\"Downloading dataset...\")\n",
        "    # Using a reliable raw text file from the Tatoeba project via a stable GitHub source\n",
        "    url = \"/content/deu_update.txt\"\n",
        "    try:\n",
        "        #response = requests.get(url)\n",
        "        #response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        file_path = 'eng-spa.txt'\n",
        "        with open(url, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Dataset downloaded to {file_path}\")\n",
        "        return file_path\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Preprocesses a sentence by cleaning the text and adding start/end tokens.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The raw sentence string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed sentence.\n",
        "    \"\"\"\n",
        "    # Normalize unicode characters to simplify the text\n",
        "    sentence = ''.join(c for c in unicodedata.normalize('NFD', sentence) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    sentence = sentence.lower().strip()\n",
        "    # Create a space between a word and the punctuation following it\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    # Replace all characters except letters, punctuation, and spaces\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    # Add start and end tokens\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "    \"\"\"\n",
        "    Creates a dataset from a text file, limiting it to a specific number of examples.\n",
        "\n",
        "    Args:\n",
        "        path (str): The file path to the parallel corpus.\n",
        "        num_examples (int): The number of sentence pairs to load.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing lists of preprocessed input and target sentences.\n",
        "    \"\"\"\n",
        "    print(\"Creating dataset from file...\")\n",
        "    with io.open(path, encoding='UTF-8') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "\n",
        "    # The dataset format is \"English_sentence\\tSpanish_sentence\"\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
        "\n",
        "    # We'll use Spanish as the input and English as the target.\n",
        "    # The pairs are stored as (English, Spanish) so we swap them\n",
        "    inp_lang = [pair[1] for pair in word_pairs]\n",
        "    targ_lang = [pair[0] for pair in word_pairs]\n",
        "\n",
        "    return inp_lang, targ_lang\n",
        "\n",
        "# --- Model Components ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, self.hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, values):\n",
        "        # query_with_time_axis shape: (batch_size, 1, hidden_size)\n",
        "        query_with_time_axis = query.unsqueeze(1)\n",
        "\n",
        "        # score shape: (batch_size, max_length, 1)\n",
        "        score = self.V(torch.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape: (batch_size, max_length, 1)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "\n",
        "        # context_vector shape: (batch_size, hidden_size)\n",
        "        context_vector = torch.sum(attention_weights * values, dim=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embedding_dim, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim + hidden_size, self.hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
        "        self.attention = BahdanauAttention(self.hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after embedding: (batch_size, 1, embedding_dim)\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # concatenated_input shape: (batch_size, 1, embedding_dim + hidden_size)\n",
        "        concatenated_input = torch.cat((context_vector.unsqueeze(1), embedded), dim=-1)\n",
        "\n",
        "        output, hidden = self.gru(concatenated_input, hidden.unsqueeze(0))\n",
        "\n",
        "        # output shape: (batch_size * 1, hidden_size)\n",
        "        output = output.squeeze(1)\n",
        "\n",
        "        # predictions shape: (batch_size, output_vocab_size)\n",
        "        predictions = self.fc(output)\n",
        "\n",
        "        return predictions, hidden.squeeze(0), attention_weights\n",
        "\n",
        "# --- Training and Translation Functions ---\n",
        "\n",
        "def train_step(encoder, decoder, optimizer, criterion, input_tensor, target_tensor, max_length_target, targ_lang_tokenizer):\n",
        "    encoder_hidden = encoder.init_hidden(input_tensor.size(0))\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "    decoder_input = torch.tensor([[targ_lang_tokenizer['word_to_idx']['<start>']]] * input_tensor.size(0), device=device)\n",
        "    decoder_hidden = encoder_hidden.squeeze(0)\n",
        "\n",
        "    loss = 0\n",
        "    for t in range(target_tensor.size(1)):\n",
        "        predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "\n",
        "        # Mask out padding tokens from loss calculation\n",
        "        mask = target_tensor[:, t].ne(0).float()\n",
        "        loss += criterion(predictions, target_tensor[:, t].to(device)) * mask\n",
        "\n",
        "        decoder_input = target_tensor[:, t].unsqueeze(1)\n",
        "\n",
        "    # Calculate average loss per sequence\n",
        "    loss /= target_tensor.size(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.sum().backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.mean().item()\n",
        "\n",
        "def translate(sentence, encoder, decoder, inp_lang_tokenizer, targ_lang_tokenizer, max_length_input, max_length_target):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    try:\n",
        "        inputs = [inp_lang_tokenizer['word_to_idx'][i] for i in sentence.split(' ')]\n",
        "    except KeyError as e:\n",
        "        return f\"Error: Word '{e}' not found in Spanish vocabulary. Please try a different sentence.\"\n",
        "\n",
        "    inputs = torch.tensor(inputs, device=device).unsqueeze(0)\n",
        "\n",
        "    encoder_hidden = encoder.init_hidden(1)\n",
        "    encoder_output, encoder_hidden = encoder(inputs, encoder_hidden)\n",
        "\n",
        "    decoder_hidden = encoder_hidden.squeeze(0)\n",
        "    decoder_input = torch.tensor([[targ_lang_tokenizer['word_to_idx']['<start>']]], device=device)\n",
        "\n",
        "    translated_words = []\n",
        "\n",
        "    for _ in range(max_length_target):\n",
        "        predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "\n",
        "        predicted_id = predictions.argmax(1).item()\n",
        "        translated_word = targ_lang_tokenizer['idx_to_word'][predicted_id]\n",
        "\n",
        "        if translated_word == '<end>':\n",
        "            break\n",
        "\n",
        "        translated_words.append(translated_word)\n",
        "        decoder_input = torch.tensor([[predicted_id]], device=device)\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    return ' '.join(translated_words)\n",
        "\n",
        "# --- Custom Dataset and Vocab for PyTorch ---\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, inp_lang, targ_lang):\n",
        "        self.inp_lang = inp_lang\n",
        "        self.targ_lang = targ_lang\n",
        "\n",
        "        # Manually build the vocabularies to avoid torchtext issues\n",
        "        self.inp_vocab = self.build_vocabulary(self.inp_lang)\n",
        "        self.targ_vocab = self.build_vocabulary(self.targ_lang)\n",
        "\n",
        "    def build_vocabulary(self, sentences):\n",
        "        \"\"\"\n",
        "        Builds a word-to-index and index-to-word mapping from a list of sentences.\n",
        "        \"\"\"\n",
        "        word_to_idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
        "        idx_to_word = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.split():\n",
        "                if word not in word_to_idx:\n",
        "                    word_to_idx[word] = idx\n",
        "                    idx_to_word[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "        return {'word_to_idx': word_to_idx, 'idx_to_word': idx_to_word, 'size': len(word_to_idx)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inp_lang)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inp_lang[idx], self.targ_lang[idx]\n",
        "\n",
        "def pad_collate_fn(batch, inp_vocab, targ_vocab):\n",
        "    inp_list = []\n",
        "    targ_list = []\n",
        "\n",
        "    for inp, targ in batch:\n",
        "        inp_list.append([inp_vocab['word_to_idx'].get(word, inp_vocab['word_to_idx']['<unk>']) for word in inp.split()])\n",
        "        targ_list.append([targ_vocab['word_to_idx'].get(word, targ_vocab['word_to_idx']['<unk>']) for word in targ.split()])\n",
        "\n",
        "    # PyTorch's pad_sequence pads to the longest sequence in the batch\n",
        "    padded_inp = nn.utils.rnn.pad_sequence([torch.tensor(x, dtype=torch.long) for x in inp_list], batch_first=True, padding_value=inp_vocab['word_to_idx']['<pad>'])\n",
        "    padded_targ = nn.utils.rnn.pad_sequence([torch.tensor(x, dtype=torch.long) for x in targ_list], batch_first=True, padding_value=targ_vocab['word_to_idx']['<pad>'])\n",
        "\n",
        "    return padded_inp, padded_targ\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. Prepare the dataset\n",
        "    #file_path = download_data()\n",
        "    #if file_path is None:\n",
        "    #    print(\"Could not download the dataset. Exiting.\")\n",
        "    #else:\n",
        "    if 1:\n",
        "        inp_lang, targ_lang = create_dataset('/content/deu_update.txt', num_examples)\n",
        "\n",
        "        # Create a PyTorch Dataset and DataLoader\n",
        "        dataset = TranslationDataset(inp_lang, targ_lang)\n",
        "        inp_vocab = dataset.inp_vocab\n",
        "        targ_vocab = dataset.targ_vocab\n",
        "        inp_vocab_size = inp_vocab['size']\n",
        "        targ_vocab_size = targ_vocab['size']\n",
        "\n",
        "        BATCH_SIZE = 64\n",
        "        data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                 collate_fn=lambda x: pad_collate_fn(x, inp_vocab, targ_vocab))\n",
        "\n",
        "        max_length_targ = max(len(s.split()) for s in targ_lang)\n",
        "        max_length_inp = max(len(s.split()) for s in inp_lang)\n",
        "\n",
        "        print(\"\\nData preparation complete.\")\n",
        "        print(\"Max length of input sentences:\", max_length_inp)\n",
        "        print(\"Max length of target sentences:\", max_length_targ)\n",
        "        print(\"Size of input vocabulary:\", inp_vocab_size)\n",
        "        print(\"Size of target vocabulary:\", targ_vocab_size)\n",
        "\n",
        "        # 2. Build the model\n",
        "        print(\"\\nBuilding the model...\")\n",
        "        encoder = Encoder(inp_vocab_size, embedding_dim, hidden_size).to(device)\n",
        "        decoder = Decoder(targ_vocab_size, embedding_dim, hidden_size).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=targ_vocab['word_to_idx']['<pad>'], reduction='none')\n",
        "\n",
        "        # 3. Train the model\n",
        "        print(\"Training the model...\")\n",
        "        EPOCHS = 5\n",
        "        for epoch in range(EPOCHS):\n",
        "            total_loss = 0\n",
        "            for batch_idx, (input_tensor, target_tensor) in enumerate(data_loader):\n",
        "                input_tensor = input_tensor.to(device)\n",
        "                target_tensor = target_tensor.to(device)\n",
        "\n",
        "                loss = train_step(encoder, decoder, optimizer, criterion, input_tensor, target_tensor, max_length_targ, targ_vocab)\n",
        "                total_loss += loss\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss:.4f}\")\n",
        "\n",
        "            print(f\"Epoch {epoch + 1} finished. Avg Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "        print(\"\\nTraining complete.\")\n",
        "\n",
        "        # 4. Save the model\n",
        "        # Create a folder to save the models if it doesn't exist\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "\n",
        "        torch.save(encoder.state_dict(), 'models/encoder.pth')\n",
        "        torch.save(decoder.state_dict(), 'models/decoder.pth')\n",
        "\n",
        "        print(\"Model saved successfully as 'models/encoder.pth' and 'models/decoder.pth'\")\n",
        "\n",
        "        # 5. Demonstrate translation\n",
        "        print(\"\\nDemonstrating translation of example sentences:\")\n",
        "        example_sentences = [\n",
        "            'Kennen Sie sie',\n",
        "            'Sehen Sie das',\n",
        "            'Zählt das'\n",
        "        ]\n",
        "\n",
        "        for sentence in example_sentences:\n",
        "            translated_sentence = translate(sentence, encoder, decoder, inp_vocab, targ_vocab, max_length_inp, max_length_targ)\n",
        "            print(f\"\\nInput: {sentence}\")\n",
        "            print(f\"Translated: {translated_sentence}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset from file...\n",
            "\n",
            "Data preparation complete.\n",
            "Max length of input sentences: 13\n",
            "Max length of target sentences: 9\n",
            "Size of input vocabulary: 3531\n",
            "Size of target vocabulary: 2209\n",
            "\n",
            "Building the model...\n",
            "Training the model...\n",
            "Epoch 1, Batch 0, Loss: 5.8860\n",
            "Epoch 1, Batch 10, Loss: 2.4905\n",
            "Epoch 1, Batch 20, Loss: 2.5202\n",
            "Epoch 1, Batch 30, Loss: 2.2795\n",
            "Epoch 1, Batch 40, Loss: 2.0400\n",
            "Epoch 1, Batch 50, Loss: 1.5770\n",
            "Epoch 1, Batch 60, Loss: 1.7602\n",
            "Epoch 1, Batch 70, Loss: 1.5914\n",
            "Epoch 1, Batch 80, Loss: 1.7128\n",
            "Epoch 1, Batch 90, Loss: 1.4655\n",
            "Epoch 1, Batch 100, Loss: 1.5485\n",
            "Epoch 1, Batch 110, Loss: 1.4124\n",
            "Epoch 1, Batch 120, Loss: 1.3748\n",
            "Epoch 1, Batch 130, Loss: 1.2541\n",
            "Epoch 1, Batch 140, Loss: 1.4025\n",
            "Epoch 1, Batch 150, Loss: 1.4120\n",
            "Epoch 1 finished. Avg Loss: 1.8165\n",
            "Epoch 2, Batch 0, Loss: 1.0287\n",
            "Epoch 2, Batch 10, Loss: 0.9546\n",
            "Epoch 2, Batch 20, Loss: 0.9630\n",
            "Epoch 2, Batch 30, Loss: 1.0273\n",
            "Epoch 2, Batch 40, Loss: 0.9417\n",
            "Epoch 2, Batch 50, Loss: 0.9862\n",
            "Epoch 2, Batch 60, Loss: 1.0596\n",
            "Epoch 2, Batch 70, Loss: 0.8708\n",
            "Epoch 2, Batch 80, Loss: 0.9324\n",
            "Epoch 2, Batch 90, Loss: 0.8359\n",
            "Epoch 2, Batch 100, Loss: 0.8498\n",
            "Epoch 2, Batch 110, Loss: 0.9175\n",
            "Epoch 2, Batch 120, Loss: 0.7618\n",
            "Epoch 2, Batch 130, Loss: 0.7966\n",
            "Epoch 2, Batch 140, Loss: 0.7912\n",
            "Epoch 2, Batch 150, Loss: 0.8150\n",
            "Epoch 2 finished. Avg Loss: 0.9257\n",
            "Epoch 3, Batch 0, Loss: 0.5672\n",
            "Epoch 3, Batch 10, Loss: 0.4988\n",
            "Epoch 3, Batch 20, Loss: 0.4070\n",
            "Epoch 3, Batch 30, Loss: 0.4433\n",
            "Epoch 3, Batch 40, Loss: 0.5538\n",
            "Epoch 3, Batch 50, Loss: 0.4889\n",
            "Epoch 3, Batch 60, Loss: 0.5251\n",
            "Epoch 3, Batch 70, Loss: 0.5304\n",
            "Epoch 3, Batch 80, Loss: 0.4098\n",
            "Epoch 3, Batch 90, Loss: 0.5027\n",
            "Epoch 3, Batch 100, Loss: 0.4206\n",
            "Epoch 3, Batch 110, Loss: 0.5409\n",
            "Epoch 3, Batch 120, Loss: 0.4647\n",
            "Epoch 3, Batch 130, Loss: 0.4977\n",
            "Epoch 3, Batch 140, Loss: 0.5589\n",
            "Epoch 3, Batch 150, Loss: 0.4534\n",
            "Epoch 3 finished. Avg Loss: 0.4819\n",
            "Epoch 4, Batch 0, Loss: 0.2120\n",
            "Epoch 4, Batch 10, Loss: 0.2274\n",
            "Epoch 4, Batch 20, Loss: 0.2421\n",
            "Epoch 4, Batch 30, Loss: 0.2304\n",
            "Epoch 4, Batch 40, Loss: 0.1541\n",
            "Epoch 4, Batch 50, Loss: 0.2063\n",
            "Epoch 4, Batch 60, Loss: 0.1909\n",
            "Epoch 4, Batch 70, Loss: 0.2267\n",
            "Epoch 4, Batch 80, Loss: 0.1904\n",
            "Epoch 4, Batch 90, Loss: 0.2861\n",
            "Epoch 4, Batch 100, Loss: 0.2035\n",
            "Epoch 4, Batch 110, Loss: 0.2626\n",
            "Epoch 4, Batch 120, Loss: 0.3398\n",
            "Epoch 4, Batch 130, Loss: 0.2592\n",
            "Epoch 4, Batch 140, Loss: 0.2557\n",
            "Epoch 4, Batch 150, Loss: 0.2736\n",
            "Epoch 4 finished. Avg Loss: 0.2389\n",
            "Epoch 5, Batch 0, Loss: 0.1344\n",
            "Epoch 5, Batch 10, Loss: 0.1108\n",
            "Epoch 5, Batch 20, Loss: 0.1030\n",
            "Epoch 5, Batch 30, Loss: 0.1223\n",
            "Epoch 5, Batch 40, Loss: 0.1178\n",
            "Epoch 5, Batch 50, Loss: 0.1257\n",
            "Epoch 5, Batch 60, Loss: 0.1649\n",
            "Epoch 5, Batch 70, Loss: 0.1699\n",
            "Epoch 5, Batch 80, Loss: 0.1569\n",
            "Epoch 5, Batch 90, Loss: 0.1525\n",
            "Epoch 5, Batch 100, Loss: 0.1546\n",
            "Epoch 5, Batch 110, Loss: 0.1127\n",
            "Epoch 5, Batch 120, Loss: 0.1700\n",
            "Epoch 5, Batch 130, Loss: 0.1488\n",
            "Epoch 5, Batch 140, Loss: 0.1054\n",
            "Epoch 5, Batch 150, Loss: 0.1457\n",
            "Epoch 5 finished. Avg Loss: 0.1391\n",
            "\n",
            "Training complete.\n",
            "Model saved successfully as 'models/encoder.pth' and 'models/decoder.pth'\n",
            "\n",
            "Demonstrating translation of example sentences:\n",
            "\n",
            "Input: Cómo estás?\n",
            "Translated: Error: Word ''como'' not found in Spanish vocabulary. Please try a different sentence.\n",
            "\n",
            "Input: Me gusta el sol.\n",
            "Translated: Error: Word ''me'' not found in Spanish vocabulary. Please try a different sentence.\n",
            "\n",
            "Input: Eso es un libro.\n",
            "Translated: Error: Word ''eso'' not found in Spanish vocabulary. Please try a different sentence.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxitRZPBc0ni",
        "outputId": "2e6db9f3-3fec-464a-8013-46375a16fdd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Demonstrate translation\n",
        "print(\"\\nDemonstrating translation of example sentences:\")\n",
        "example_sentences = [\n",
        "    'Kennen Sie sie',\n",
        "    'Sehen Sie das',\n",
        "    'Zählt das'\n",
        "]\n",
        "\n",
        "for sentence in example_sentences:\n",
        "    translated_sentence = translate(sentence, encoder, decoder, inp_vocab, targ_vocab, max_length_inp, max_length_targ)\n",
        "    print(f\"\\nInput: {sentence}\")\n",
        "    print(f\"Translated: {translated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESWhzJmkeqH5",
        "outputId": "a5b01b55-f9f5-44e2-f731-3e303f1835dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Demonstrating translation of example sentences:\n",
            "\n",
            "Input: Kennen Sie sie\n",
            "Translated: <start> do you know .\n",
            "\n",
            "Input: Sehen Sie das\n",
            "Translated: <start> take that .\n",
            "\n",
            "Input: Zählt das\n",
            "Translated: <start> take this .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mO5GQJCokYhi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}